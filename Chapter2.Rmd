---
title: "Chapter 2"
author: "GB"
date: "2/24/2022"
output:
  html_document: 
    mathjax: default
header-includes: \usepackage{xcolor}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## What is conditional probability?

Lets say you want to know if a random day in the year is a winter day, is a snow day, and what the relationship between winter and snow days is.

We start by drawing a circle, which symbolizes all days in a year:

```{r, echo = F}
library(plotrix)
par(mar = c(0,0,0,0))
pie(c(3,1), labels = "")
draw.circle(0,0,.8, col ="white")
```

We know that a quarter of the days in a year are in winter months. We visualize this by highlighting a section of the circle in icy blue.

```{r, echo = F}
par(mar = c(0,0,0,0))
pie(c(3,1), labels = "")
```

From this we can see that the probability of winter is 1/4, or 0.25. This is an unconditional probability, and we can write $P(winter) = 0.25$.

Snow happens mostly in winter, so we add an ellipse that shows that snowing days are typically in the winter:

```{r, echo = F}
par(mar = c(0,0,0,0))
pie(c(3,1), labels = "")
e.rot = 45
h.e = .25
k.e = -.25
a.e = .5
b.e = .175
draw.ellipse(h.e,k.e,a.e,b.e , col = adjustcolor("grey",alpha = .75), angle = e.rot)
```

The unconditional probability of snow, $P(snow)$, is the probability of snow independent if it is winter or not, and corresponds to the area of the grey ellipse. The probability of snow is the area of the ellipse divided by the area of the circle, and computes to $P(snow) =$ 
`r pi*a.e*b.e/(pi*.8^2)`.

We can also use the figure above to explain what a joint probability is: The joint probability of winter and snow $P(winter, snow)$ or $P(winter \cap snow)$ is the ellipse-area that overlaps with the bottom right quadrant.

What is a conditional probability? A conditional probability gives an answer to the question "Given that it is winter, what is the probability of snow". In the figure above this amounts to the question "What is the size of the ellipse-area that overlaps with the bottom right quadrant, _relative to_ the area of the bottom right quadrant". We are "conditioning" our question about the probability of snow on the value of another variable, namely that the season is winter. In comparison, the unconditional probability asks "What is the size of the ellipse-area, _relative to_ the total circle area".

So, to calculate the conditional probability $P(snow|winter)$ we want to answer the question "What is the size of the ellipse-area that overlaps with the bottom right quadrant, _relative to_ the area of the bottom right quadrant". Unfortunately, there is no simple equation for calculating the overlap of an ellipse and a section of a circle. But we can approximate the correct answer by randomly placing points into the circle and checking if they are in the bottom right quadrant (winter), in the ellipse (snow), or in the area of the ellipse that overlaps with the bottom right quadrant (winter and snow).

Here is the code for the simulation. You don't need to understand all of it, the important part is that we are keeping track of winter and snow days.

```{r, echo = F}
shift_rotate = function(xy,shift_x, shift_y, angle) {
  theta = 2*pi/(1/(angle/360))
  trans.mat = transf = matrix(c(cos(theta),-sin(theta),sin(theta),cos(theta)),nrow = 2)
  
  x.e = xy[1] - shift_x
  y.e = xy[2] - shift_y
  
  xy_transf = trans.mat %*% matrix(c(x.e,y.e))
  
  return(xy.r = c(
    x.r = shift_x + xy_transf[1],
    y.r = shift_y + xy_transf[2]  
  ))
}

in_ellipse = function(xy,h,k,a,b, e.rot) {
  xy.r = shift_rotate(xy,h,k,e.rot)
  (xy.r[1]-h)^2/a^2 + (xy.r[2]-k)^2/b^2 <= 1
}

rpoint_in_circle = function() {
  r = 0.8 * sqrt(runif(1))
  theta = runif(1) * 2 * pi
  return(c(0 + r * cos(theta),
           0 + r * sin(theta)))
}

```


```{r}
set.seed(123)
par(mar = c(0,0,0,0))
pie(c(3,1), labels = "")
draw.ellipse(h.e,k.e,a.e,b.e , col = adjustcolor("grey",alpha = .75), angle = e.rot)

N = 365
is.winter = vector(length = N) # vector to count winter days
is.snow = vector(length = N) # vector to count snow days

for (k in 1:N) {
  # generate random point with custom function
  xy = rpoint_in_circle()
  
  # check if it is a snow day, i.e. in ellipse, with custom function
  is.snow[k] = in_ellipse(xy,h.e,k.e,a.e,b.e,e.rot)
  # check if it is a winter day
  is.winter[k] = xy[1] > 0 & xy[2] < 0
  
  # plot points
  points(xy[1],xy[2],
         pch = ifelse(is.winter[k] == T,23,21),
         bg = ifelse(is.snow[k] == T,"green4","red"),
         col = ifelse(is.snow[k] == T,"green4","red"))

}

legend("topright",
       pch = c(18,16,15,15),
       col = c("black","black","green4","red"),
       legend = c("winter","no-winter", "snow", "no-snow"))
```

Let's first calculate the probability of winter, which should be around 0.25. This is simply the number of diamond-shaped dots divided by the total number of dots.

```{r}
N_winter = sum(is.winter)
P_winter = N_winter/N
P_winter
```

Now, the probability of snow (green dots divided by total number of dots):

```{r}
N_snow = sum(is.snow)
P_snow = N_snow/N
P_snow
```
And now it gets interesting. For the joint probability of winter and snow $P(winter, snow)$ we count the number of green diamonds. 

```{r}
# logical indexing:
# is.snow[is.winter] returns only those entries of the 
# vector is.snow that are at positions where the 
# value for is.winter is TRUE
N_winter_and_snow = sum(is.snow[is.winter]) 
P_winter_and_snow = N_winter_and_snow/N
P_winter_and_snow
```

Check the last code block and see that to get the (unconditional) joint probability we divide by the total number of dots `N`.

In contrast, for conditional probabilities, we want to divide by the number of dots that have the value we are conditioning on. If we want to calculate the conditional probability $P(snow | winter)$, we therefore have to divide by the number of winter dots:

```{r}
P_snow_given_winter = N_winter_and_snow/N_winter
P_snow_given_winter
```
If you check further above, you can see that `P_winter_and_snow` and `P_winter` are calculated by dividing `N_winter_and_snow` and `N_winter` with `N`, respectively. Therefore `N_winter_and_snow/N_winter` and `P_winter_and_snow/P_winter` have the same result, and we can also write:

```{r}
P_snow_given_winter = P_winter_and_snow/P_winter
```

Hopefully, you recognize now that we have the conditional probability on the left side (`P_snow_given_winter` or $P(snow|winter)$), which we calculate with help of the joint probability (`P_winter_and_snow` or $P(snow, winter)$) and the unconditional probability (`P_winter` or $P(winter)$) on the right side:

$$
P(snow|winter) = \frac{P(snow, winter)}{P(winter)}
$$

or more abstract:

$$
P(A|B) = \frac{P(A, B)}{P(B)}
$$
and by multiplying with $P(B)$ on both sides, we get 

$$
P(A, B) = P(A|B) \cdot P(B)
$$

which is the chain rule that connects conditional probabilities with joint probabilities.

Can you also show that the following is true:

$$
P(A, B) = P(B|A) \cdot P(A)
$$

Or, using the example above

$$
P(snow, winter) = P(winter|snow) \cdot P(snow)
$$

## 2E1 Probabaility of rain on Monday

2 and 4 are correct. 

Why 4, P(rain, Monday) / P(Monday)?

We know generally that 

P(A,B) = P(A|B) * P(B) (page 37)

then 

P(rain, Monday) = P(rain | Monday) * P(Monday)

divide by P(Monday) on both sides

P(rain, Monday) / P(Monday) = P(rain | Monday)

## 2E2 What does Pr(Monday|rain) mean?

Probability of rain, given that it is Monday

3 is correct

## 2E3 Consistent with "probability of Monday given that it is raining"

1 and 4 are correct. Why 4?

1: P(Monday|rain)

4: P(rain|Monday) * P(Monday) / P(rain)

The question is then if we can show that 

$P(Monday|rain) = P(rain|Monday) \cdot P(Monday) / P(rain)$?

The key to the solution is that we can use both $P(Monday|rain)$ and $P(rain|Monday)$ to calculate the same thing: the joint probability of $P(Monday,rain)$.

Page 37 shows the relationship of joint and conditional probability:

$$
P(A,B) = \color{blue}{P(A|B) \cdot P(B)} \\
P(A,B) = \color{red}{P(B|A) \cdot P(A)}
$$

Therefore, we can say that the joint probability that it is Monday and raining is 

$$P(Monday,rain) = \color{blue}{P(rain|Monday) \cdot P(Monday)}$$

or

$$P(Monday,rain) = \color{red}{P(Monday|rain) \cdot P(rain)}$$
and we can further say 

$$
\color{red}{P(Monday|rain) \cdot P(rain)} = \color{blue}{P(rain|Monday) \cdot P(Monday)}
$$

If we want to know what P(Monday|rain) is, we now have to devide on both sides with P(rain), which gives us

$$
\color{red}{P(Monday|rain)} = \frac{\color{blue}{P(rain|Monday) \cdot P(Monday)}}{\color{red}{P(rain)}}
$$

where the right hand side is answer 4 from above.


## 2E4 What does it mean to say "the probability of water is 0.7"?

It seems fair to describe Richard McElreath as a determinist which is 

> the philosophical view that all events are determined completely by previously existing causes^[https://en.wikipedia.org/wiki/Determinism]

This means that he takes the position^[in good company of many philosophers including Aristotle, Hobbes, Laplace or Daniel Dennet and physicists like Bohr and Einstein] that in principle everything is with certainty explainable, provided one has access to and can effectively use all relevant information. If everything is explainable, things happen or they don't, and therefor probability does not exist in nature.

However, limited information (and lack of Leviathan information processing) brings probability into existence for humans.

If a globe-tossing person would have all relevant information and computing power to calculate how a tossed globe moves through the air and is caught, she could say with certainty if the index finger would land on water and land, and therefore the probability of water would always be 1 or 0. Globe tossers who do not have the relevant information and computing power remain uncertain and express their uncertainty by stating with what probability the index finger will lang on water.

One consequence of this view is that the same situation can be deterministic for one person, and yet random for another person. For instance, random numbers generated with a computer are truly uncertain for somebody who does not know the "seed" number and the algorithm used to generate a random number. Such a person should make probabilistic statements about random numbers from the computer. In contrast, a person that know seed number and algorithm can make deterministic statements. For example, I know with certainty that if you execute the following command in R

```{r, eval = F}
set.seed(123)
runif(1) # random number between 0 and 1
```

that the resulting number will be smaller than 0.5. Other can only give a probability that the number is smaller than 0.5.

Probabilistic statements as expression of uncertainty can be related to, but are not the same as frequency-based statements. Frequency based statements come always from a countable event type: One has to count over multiple occurrences of an event to obtain frequencies. Thus, one could also arrive at the statement "the probability of water is 0.7" by tossing the globe multiple times. But we do not need to, we can also arrive at a probabilistic statement by starting with the fact that 70% of the earths surface is water.

Allowing probabilistic statements also for non-countable events is not uncontroversial. For instance, the classical NHST framework uses only frequencies. One motivation to still allow or use probabilistic statements about one-time events is that they make it possible to express uncertainty about one time events, e.g. the probability that we will have more then 5 hours cloudless sunshine one day from today in Oslo.


## 2M1 Grid posterior

### W-W-W
You need to use a sufficiently fine-grained grid to make differences visible.

```{r 2M1a}
# parameter values at which we calculate the posterior probability
p_H2O_grid = seq(from=0 , to=1, length.out=100)
# likelihood of 3 water in 3 tosses
N_tosses = 3
N_water = 3
likelihood = dbinom(N_water , size=N_tosses, prob=p_H2O_grid)
# uniform prior
prior = rep(1,100) 
posterior = likelihood * prior
sum(posterior)
plot(p_H2O_grid, posterior , type="l" , ylab = "density", xlab = "p")
# standardize 
posterior = posterior / sum(posterior)
sum(posterior)
```

We use R's `plot` command to show the results:
```{r 2M1b}
plot(p_H2O_grid, posterior , type="l", ylab = "density", xlab = "p" )
# this also works
# plot( posterior ~ p_H2O_grid , type="l" )
# personally, I prefer the plot(x_values, y_values) syntax
```

### W-W-W-L
For 4 tosses and 3 W.
```{r 2M1c}
likelihood = dbinom( 3 , size=4 , prob=p_H2O_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l" , ylab = "density", xlab = "p")
```

### W-W-W-W-W-L-L
For 7 tosses and 5 W.
```{r 2M1d}
likelihood = dbinom( 5 , size=7 , prob=p_H2O_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l" )
```

## 2M2 Modified priors

```{r 2M2}
prior[p_H2O_grid < .5] = 0

posterior = dbinom( 3 , size=3 , prob=p_H2O_grid ) * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l" , ylab = "density", xlab = "p")

posterior = dbinom( 3 , size=4 , prob=p_H2O_grid ) * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l" , ylab = "density", xlab = "p")

posterior = dbinom( 5 , size=7 , prob=p_H2O_grid ) * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l", ylab = "density", xlab = "p" )

``` 

## 2M3 Pr(Earth|land)

P(Earth | land) = .23 ? 

We know that 

- P(land | Earth) = 1 - P(water | Eart) = .3
- P(land | Mars) = 1
- P(Earth) = .5

We want to use
$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$
A = Earth
B = land

so

$$
P(Earth|land) = \frac{P(land|Earth) \cdot P(Earth)}{P(land)}
$$

We know

- P(B|A) = P(land | Earth) = .3
- P(A) = P(Earth) = .5

But we are missing P(B) = P(land)

P(land) = (P(land | Earth) + P(land | Mars)) / 2 = .65


P(Earth | land) = P(land | Earth) * P(Earth) / P(land)

```{r 2M3}
.3 * .5 / .65
```

## 2M4 Probability that the back side is black with decks BB, BW, WW

We make a table with possible outcomes
```{r 2M4a, message=FALSE, warning=FALSE}
library(dplyr)  # for data manipulation
library(magrittr) # for pipes
library(knitr) # to show tables in Markdown
library(kableExtra) # for prettier tables
counts = 
  data.frame(
    Up =   c("B","B","B","W","W","w"),
    Down = c("B","B","W","B","W","W"))

kable(counts) %>% 
  kable_styling(full_width = F)
```

We keep only the rows where the up side is black and count how many of those are also black on the other side.

```{r 2M4b, message=FALSE}
counts %>% 
  filter(Up == "B") %>% 
  pull(Down) %>% 
  table() %>% 
  prop.table()
```

Just for fun, here is the data.table way:

```{r 2M4c, message=FALSE}
library(data.table)
data.table(
  Up =   c("B","B","B","W","W","w"),
  Down = c("B","B","W","B","W","W"))  %>%
  .[Up == "B"] %>% # dplyr: filter(Up == "B")
  .[, Down] %>%    # dplyr: pull(Down) %>% 
  table() %>% 
  prop.table()
```

## 2M5 Probability that the back side is black with decks BB, BB, BW, WW

We are just adding one BB card

```{r 2M5, message=FALSE}
library(data.table)
data.table(
  Up =   c("B","B","B","B","B","W","W","w"),
  Down = c("B","B","B","B","W","B","W","W"))  %>%
  .[Up == "B"] %>% 
  .[, Down] %>% 
  table()  %>% 
  prop.table()
```

## 2M6 Probability that the back side is black with heavy black

We use the `rep` command to manipulate the probability with which the different cards are chosen.

```{r 2M6, message=FALSE}
library(data.table)
data.table(
  Up =   c(rep(c("B","B"),1),
           rep(c("B","W"),2),
           rep(c("W","w"),3)),
  Down = c(rep(c("B","B"),1),
           rep(c("W","B"),2),
           rep(c("W","w"),3)))  %>%
  .[Up == "B"] %>% 
  .[, Down] %>% 
  table() %>% 
  prop.table()
```

## 2M7 Probability that the back side is black given that a 2nd card shows white

There are 3 ways to draw a black face up: 

- Face 1 from BB
- Face 2 from BB
- Face 1 from BW

When we start with a black face from BB, there are three ways to have a white face up on the second card:

- Face 1 from WW
- Face 2 from WW
- Face 2 from BW

When we start with a black face from BW, there are two ways to have a white face up on the second card:

- Face 1 from WW
- Face 2 from WW

We are using the `expand.grid` function to generate combinations of draws.

```{r 2M7a}
BB_ways = 
  expand.grid(Black_up = c("BB","BB"),
              White_up = c("WB","WW","WW"))

BW_ways = 
  expand.grid(Black_up = c("BW"),
              White_up = c("WW","WW"))

all_ways = 
  rbind(
    BB_ways,
    BW_ways
  ) 

kable(all_ways) %>% 
  kable_styling(full_width = F)
  
```

and we count how often the down face of the "Black_up" cards is also black.

```{r 2M7b}
all_ways %>% 
  data.frame() %>% 
  rowwise() %>% 
  mutate(face_down = substr(Black_up,2,2)) %>% 
  pull(face_down) %>% 
  table() %>% 
  prop.table()
```

_COMMENT: ARE THERE OTHER “EASIER” SOLUTIONS TO GET THE SAME CONCLUSION????_

GB: Not any I can immediately think of.

# Clarification questions

When to use probability versus plausibility? Are these words interchangeable? What about Pr vs. p?

Can you say something more about the usage of words “likelihood” and “distribution function” in Bayesian vs frequentist statistics? (ref section 2.3.2.1 on p. 33)


# Exercises for next time:

Everyone 3E1 - 3M6

Why so many excersises?

- 3E1 - 3E7 are easy / fast.
- 3M1 - 3M3 Build on each other
- 3M4, 3M5 just re-use the code from 3M1 - 3M3


General comments:

Please ...

- add the name of the team as author
- try to be more wordy, explain your code and/or use declarative variable names. (It is not suffient just to write the answers ;-))
- try to write down the solutions so that it will be easy to explain to others what you did.
- please submit compiled html files and not R-Markdown files (and check if the compiled html looks OK: line separation, plots shown etc)
- send your asnwers only once
- next time, every team  submit a clarification question or a discussion question

Rpubs:

- I propose we try to use Rpubs (https://rpubs.com/), i.e. you can submit a link to an Rpubs site. You can delete you post after the class.