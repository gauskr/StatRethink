---
title: "Chapter 2"
author: "GB"
date: "2/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Teams

```{r, echo = F, warning = F}
library(magrittr)
library(knitr)
library(kableExtra)
teams = c(
  "crazy rich bayesians",
"Laplace's demons",
"(Probably) Aspiring Golem Engineers",
"Bayesic Butchers",
"The Golem's Posterior",
"EMM",
"The Statistics Owls",
"Paranormal Distribution",
"Cheerful Confounders",
"average joe's",
"The No Priors",
"R&E*",
"The Marbles*",
"the outliers*",
"We Stan*"
)

data.frame(Teams = teams) %>% 
  kable %>% 
  kable_styling(full_width = F)
```


## 2E1 Probabaility of rain on Monday

2 and 4 are correct. 

Why 4, P(rain, Monday) / P(Monday)?

We know generally that 

P(A,B) = P(A|B) * P(B) (page 37)

then 

P(rain, Monday) = P(rain | Monday) * P(Monday)

divide by P(Monday) on both sides

P(rain, Monday) / P(Monday) = P(rain | Monday)

## 2E2 What does Pr(Monday|rain) mean?

Probability of rain, given that it is Monday

3 is correct

## 2E3 Consistent with "probability of Monday given that it is raining"

1 and 4 are correct. Why 4?

1: P(Monday | rain)

4: P(rain|Monday) * P(Monday) / P(rain)


Page 37 shows the relationship of joint and conditional probability:

$$
P(A,B) = P(A|B) \cdot P(B) \\
P(A,B) = P(B|A) \cdot P(A)
$$

We can say that the joint probability that it is Monday and raining is 

$$P(Monday,rain) = P(rain|Monday) \cdot P(Monday)$$

Equally, we can say

$$P(Monday,rain) = P(Monday|rain) \cdot P(rain)$$

where we have the term $P(rain|Monday)$ we are looking for, and which we can get as $P(rain|Monday) = P(rain,Monday)/P(Monday)$. 

Because we had calculated $P(Monday,rain) = P(rain|Monday) * P(Monday)$ above and $P(Monday|rain) = P(Monday, rain) / P(rain)$, we can also say 

P(Monday,rain) = P(rain|Monday) * P(Monday) / P(rain)

## 2E4 What does it mean to say "the probability of water is 0.7"?

It seems fair to describe Richard McElreath as a determinist which is 

> the philosophical view that all events are determined completely by previously existing causes^[https://en.wikipedia.org/wiki/Determinism]

This means that he takes the position^[in good company of many philosophers including Aristotle, Hobbes, Laplace or Daniel Dennet and physicists like Bohr and Einstein] that in principle everything is with certainty explainable, provided one has access to and can effectively use all relevant information. If everything is explainable, things happen or they don't, and therefor probability does not exist in nature.

However, limited information (and lack of Leviathan information processing) brings probability into existence for humans.

If a globe-tossing person would have all relevant information and computing power to calculate how a tossed globe moves through the air and is caught, she could say with certainty if the index finger would land on water and land, and therefore the probability of water would always be 1 or 0. Globe tossers who do not have the relevant information and computing power remain uncertain and express their uncertainty by stating with what probability the index finger will lang on water.

One consequence of this view is that the same situation can be deterministic for one person, and yet random for another person. For instance, random numbers generated with a computer are truly uncertain for somebody who does not know the "seed" number and the algorithm used to generate a random number. Such a person should make probabilistic statements about random numbers from the computer. In contrast, a person that know seed number and algorithm can make deterministic statements. For example, I know with certainty that if you execute the following command in R

```{r, eval = F}
set.seed(123)
runif(1) # random number between 0 and 1
```

that the resulting number will be smaller than 0.5. Other can only give a probability that the number is smaller than 0.5.

Probabilistic statements as expression of uncertainty can be related to, but are not the same as frequency-based statements. Frequency based statements come always from a countable event type: One has to count over multiple occurrences of an event to obtain frequencies. Thus, one could also arrive at the statement "the probability of water is 0.7" by tossing the globe multiple times. But we do not need to, we can also arrive at a probabilistic statement by starting with the fact that 70% of the earths surface is water.

Allowing probabilistic statements also for non-countable events is not uncontroversial. For instance, the classical NHST framework uses only frequencies. One motivation to still allow or use probabilistic statements about one-time events is that they make it possible to express uncertainty about one time events, e.g. the probability that we will have more then 5 hours cloudless sunshine one day from today in Oslo.


## 2M1 Grid posterior

### W-W-W
You need to use a sufficiently fine-grained grid to make differences visible.

```{r 2M1a}
# parameter values at which we calculate the posterior probability
p_H2O_grid = seq(from=0 , to=1, length.out=100)
# likelihood of 3 water in 3 tosses
N_tosses = 3
N_water = 3
likelihood = dbinom(N_water , size=N_tosses, prob=p_H2O_grid)
# uniform prior
prior = rep(1,100) 
posterior = likelihood * prior
sum(posterior)
# standardize 
posterior = posterior / sum(posterior)
sum(posterior)
```

We use R's `plot` command to show the results:
```{r 2M1b}
plot(p_H2O_grid, posterior , type="l", ylab = "density", xlab = "p" )
# this also works
# plot( posterior ~ p_H2O_grid , type="l" )
# personally, I prefer the plot(x_values, y_values) syntax
```

### W-W-W-L
For 4 tosses and 3 W.
```{r 2M1c}
likelihood = dbinom( 3 , size=4 , prob=p_H2O_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l" , ylab = "density", xlab = "p")
```

### W-W-W-W-W-L-L
For 7 tosses and 5 W.
```{r 2M1d}
likelihood = dbinom( 5 , size=7 , prob=p_H2O_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l" )
```

## 2M2 Modified priors

```{r 2M2}
prior[p_H2O_grid < .5] = 0

posterior = dbinom( 3 , size=3 , prob=p_H2O_grid ) * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l" , ylab = "density", xlab = "p")

posterior = dbinom( 3 , size=4 , prob=p_H2O_grid ) * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l" , ylab = "density", xlab = "p")

posterior = dbinom( 5 , size=7 , prob=p_H2O_grid ) * prior
posterior = posterior / sum(posterior)
plot(p_H2O_grid, posterior , type="l", ylab = "density", xlab = "p" )

``` 

## 2M3 Pr(Earth|land)

P(Earth | land) = .23 ? 

We know that 

- P(land | Earth) = 1 - P(water | Eart) = .3
- P(land | Mars) = 1
- P(Earth) = .5

We want to use
$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$
A = Earth
B = land

so

$$
P(Earth|land) = \frac{P(land|Earth) \cdot P(Earth)}{P(land)}
$$

We know

- P(B|A) = P(land | Earth) = .3
- P(A) = P(Earth) = .5

But we are missing P(B) = P(land)

P(land) = (P(land | Earth) + P(land | Mars)) / 2 = .65


P(Earth | land) = P(land | Earth) * P(Earth) / P(land)

```{r 2M3}
.3 * .5 / .65
```

## 2M4 Probability that the back side is black with decks BB, BW, WW

We make a table with possible outcomes
```{r 2M4a, message=FALSE, warning=FALSE}
library(dplyr)  # for data manipulation
library(magrittr) # for pipes
library(knitr) # to show tables in Markdown
library(kableExtra) # for prettier tables
counts = 
  data.frame(
    Up =   c("B","B","B","W","W","w"),
    Down = c("B","B","W","B","W","W"))

kable(counts) %>% 
  kable_styling(full_width = F)
```

We keep only the rows where the up side is black and count how many of those are also black on the other side.

```{r 2M4b, message=FALSE}
counts %>% 
  filter(Up == "B") %>% 
  pull(Down) %>% 
  table() %>% 
  prop.table()
```

Just for fun, here is the data.table way:

```{r 2M4c, message=FALSE}
library(data.table)
data.table(
  Up =   c("B","B","B","W","W","w"),
  Down = c("B","B","W","B","W","W"))  %>%
  .[Up == "B"] %>% # dplyr: filter(Up == "B")
  .[, Down] %>%    # dplyr: pull(Down) %>% 
  table() %>% 
  prop.table()
```

## 2M5 Probability that the back side is black with decks BB, BB, BW, WW

We are just adding one BB card

```{r 2M5, message=FALSE}
library(data.table)
data.table(
  Up =   c("B","B","B","B","B","W","W","w"),
  Down = c("B","B","B","B","W","B","W","W"))  %>%
  .[Up == "B"] %>% 
  .[, Down] %>% 
  table()  %>% 
  prop.table()
```

## 2M6 Probability that the back side is black with heavy black

We use the `rep` command to manipulate the probability with which the different cards are chosen.

```{r 2M6, message=FALSE}
library(data.table)
data.table(
  Up =   c(rep(c("B","B"),1),
           rep(c("B","W"),2),
           rep(c("W","w"),3)),
  Down = c(rep(c("B","B"),1),
           rep(c("W","B"),2),
           rep(c("W","w"),3)))  %>%
  .[Up == "B"] %>% 
  .[, Down] %>% 
  table() %>% 
  prop.table()
```

## 2M7 Probability that the back side is black given that a 2nd card shows white

There are 3 ways to draw a black face up: 

- Face 1 from BB
- Face 2 from BB
- Face 1 from BW

When we start with a black face from BB, there are three ways to have a white face up on the second card:

- Face 1 from WW
- Face 2 from WW
- Face 2 from BW

When we start with a black face from BW, there are two ways to have a white face up on the second card:

- Face 1 from WW
- Face 2 from WW

We are using the `expand.grid` function to generate combinations of draws.

```{r 2M7a}
BB_ways = 
  expand.grid(Black_up = c("BB","BB"),
              White_up = c("WB","WW","WW"))

BW_ways = 
  expand.grid(Black_up = c("BW"),
              White_up = c("WW","WW"))

all_ways = 
  rbind(
    BB_ways,
    BW_ways
  ) 

kable(all_ways) %>% 
  kable_styling(full_width = F)
  
```

and we count how often the down face of the "Black_up" cards is also black.

```{r 2M7b}
all_ways %>% 
  data.frame() %>% 
  rowwise() %>% 
  mutate(face_down = substr(Black_up,2,2)) %>% 
  pull(face_down) %>% 
  table() %>% 
  prop.table()
```

_COMMENT: ARE THERE OTHER “EASIER” SOLUTIONS TO GET THE SAME CONCLUSION????_

GB: Not any I can immediately think of.

# Clarification questions

When to use probability versus plausibility? Are these words interchangeable? What about Pr vs. p?

Can you say something more about the usage of words “likelihood” and “distribution function” in Bayesian vs frequentist statistics? (ref section 2.3.2.1 on p. 33)


# Exercises for next time:

Everyone 3E1 - 3M6

Why so many excersises?

- 3E1 - 3E7 are easy / fast.
- 3M1 - 3M3 Build on each other
- 3M4, 3M5 just re-use the code from 3M1 - 3M3


General comments:

Please ...

- add the name of the team as author
- try to be more wordy, explain your code and/or use declarative variable names. (It is not suffient just to write the answers ;-))
- try to write down the solutions so that it will be easy to explain to others what you did.
- please submit compiled html files and not R-Markdown files (and check if the compiled html looks OK: line separation, plots shown etc)
- send your asnwers only once
- next time, every team  submit a clarification question or a discussion question

Rpubs:

- I propose we try to use Rpubs (https://rpubs.com/), i.e. you can submit a link to an Rpubs site. You can delete you post after the class.