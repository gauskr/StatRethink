---
title: "Chapter 4: Recap"
author: "Guido Biele"
date: "14.03.2022"
output:
  html_document: 
    mathjax: default
    toc: true
    toc_depth: 2
header-includes: 
    \usepackage{xcolor}
    \usepackage{amsmath}
---

  
```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
}
pre{
  font-size: 20px;
}
/* Headers */
  h1,h2{
    font-size: 22pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}
```

```{r setup, include=FALSE, message=FALSE, warning=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE, dpi = 300)
par(mar=c(3,3,0,1), mgp=c(1.5,.5,0), tck=-.01)
library(plotrix)
library(DescTools)
library(rethinking)
library(magrittr)
library(knitr)
library(kableExtra)
library(psych)
library(MASS)
library(png)
```

# Normal distribution & central limit theorem

Lets assume the monthly growth rate follows following distribution:

```{r CLT1, echo = F}
curve(dgamma(x,1.1,1.3),
      0,6,
      ylab = "density",
      xlab = "growth in cm per month")
```

What is the distribution of heights from 10000 people at age 16?

```{r CLT2, echo = F}
set.seed(3)
rgamma(12*16*10000,1.1,1.3) %>% 
  matrix(nrow = 10000) %>% 
  rowSums() %>% 
  hist(main = "", 
       xlab = "height in cm")
```

This is an example the displays the central limit theorem, which states that the result of processes that manifest as the sum of many small identical and independently distributed events are normally distributed.

One way to explain why this is the case is to see that there are more possible combinations of events that lead to average outcomes than possible combination of events that lead to extreme events. 

For instance, assume that you are throwing a fair coin four times, and each time heads shows you receive one credit point and each time tail shows you loos a credit point. The next table shows that there are more possible sequences that lead to an end result of 0 credit points than sequences that lead to 4 or more credit points.

```{r CLTtable, echo = F}
combs = 
  CombSet(
  c(-1,1),
  4,
  rep = T,
  ord = T)

colnames(combs) = paste("event",1:4)

combs = cbind(Permutation = 1:nrow(combs),
              combs,
              sum = rowSums(combs))

combs %>% 
  kable() %>% 
  kable_styling()
```

Now lets do the same experiment again, except that we are not looking at 4, but 16 tosses, which leads to $2^{16}$ or `r 2^16` possible sequences. Here is the distribution of credit points.

```{r CLTdist, echo = F}
combs = CombSet(
  c(-1,1),
  16,
  rep = T,
  ord = T
)

combs.sum = 
  combs %>% 
  rowSums()

h.breaks = seq(-16.01,16.01, length.out = 1+length(unique(combs.sum)))
h = 
  combs.sum %>% 
  hist(breaks = h.breaks, plot = F)

plot(h$mids, h$counts, type = 'h',
     ylab = "density",
     xlab = "number of credit points")
x = seq(-16,16, .005)
y = dnorm(x, sd = 4)
y = y/max(y)*max(h$counts)
lines(x,y, col = "grey", lty = 3)

```

One popular device to display such a process is a Galton^[Who certainly was clever, but is nowadays also infamous for his views on eugenics and race.] board:

<!-- ![Galton Board](galton1.mp4) -->

# Linear regression model

What is the association between length and weight at birth?

When data covary,we look at e.g. a scatter plot, which shows the _joint_ distribution, to see how the data are related.

```{r lreg1 , echo = F, fig.width=10}
par(mfrow = c(1,2), mar = c(0,0,0,0))
plot(0,type = "n", xaxt = "n", yaxt = "n", bty = "n",
     xlim = c(0,1), ylim = c(0,1))
Picture<-readPNG("newborn.png")
rasterImage(Picture,0,0,1,1)

par(mar=c(3,3,1,1), mgp=c(1.5,.5,0), tck=-.01)
dt = data.frame(length = rnorm(250,50,5))
expected_weight = 3.5 + scale(dt$length)*.5
dt$weight = rnorm(250,expected_weight,.5)
plot(dt,
     ylab = "weight",
     xlab = "length")
```


## What is marginalization?

Sometimes, we want information about only one dimension of the data. This information is shown in the _marginal_ distribution.

```{r marginal, echo = F, fig.width=8, fig.height=8}
scatter.hist( weight ~ length,
              data = dt,
              density = FALSE, ellipse = FALSE, smooth = F, correl = F,
              ylab = "weight",
              xlab = "length",
              col = "black", pch = 0)
```


In this plot, each histogram shows the marginal distribution of length and weight, respectively.
E.g. to get the frequency of length = 190cm, we sum all individuals with the eight, regardless of their weight.

# Modeling the mean

## Describing the model

```{r alphaModel , echo = F}
layout(matrix(c(1,1,2), nrow=1))
par(mar=c(5,5,0,0))
A = hist(dt$weight, plot = F)
ylim = c(0, max(A$breaks))
plot(dt,
     ylab = "weight",
     xlab = "length",
     ylim = ylim)
abline(h = mean(dt$weight), col = "red")
arrows(40,0,40,mean(dt$weight), col = "blue", code = 3)
text(40,1,expression(mu), pos = 4, col = "blue", cex = 2, font = 3)
par(mar=c(5,.5,0,0))
plot(NULL, type = "n", xlim = c(0, max(A$counts)), ylim = ylim, 
     bty = "n", yaxt = "n", xlab = "N", ylab = "")
rect(0,
     A$breaks[1:(length(A$breaks) - 1)], 
     A$counts, 
     A$breaks[2:length(A$breaks)],
     col = "grey")
#dy = density(dt$weight)
#plot(dy$y,dy$x)
```

To model such data, we typically think first about the likelihood function. The question here is, which distribution describes best the data we observed.
Given that we can think of birth weight as the result of a sum of many processes, a normal distribution, with parameters $\mu$ and $\sigma$ for mean and standard deviations, makes sense. (One easy way to spot parameters is that they are typically Greek letters, whereas data variables are Roman letters.)

<br />

| What | Notation | quap R-code |
|---|---|---|
|Likelihood | $w_i \sim Normal(\mu,\sigma)$ | `weight ~ dnorm(mu, sigma)` |

<br />

Parameters $\mu$ and $\sigma$ are variables that we cannot observe directly, we have to estimate them from the data _and the prior_. The table above already shows how they depend on the data, but we still need to add that they also depend on the prior:

<br />

| What | Notation | quap R-code |
|---|---|---|
|Likelihood | $w_i \sim Normal(\mu,\sigma)$ | `weight ~ dnorm(mu, sigma)` |
|Prior | $\mu \sim Normal(3.5,1.5)$ | `mu ~ dnorm(3.5, 1.5)` |
|Prior | $\sigma \sim Uniform(0,1.5)$ | `sigma ~ dunif(0, 1.5)` |

<br />

And if we want to impress (or scare off) a colleague, we can write down the full model:

$$
P(\mu,\sigma|w) = \frac{\prod_i Normal(w_i|\mu,\sigma) Normal(mu|3.5,1.5) Uniform(\sigma|0,1.5)}{\int\int \prod_i Normal(w_i|\mu,\sigma) Normal(mu|3.5,1.5) Uniform(\sigma|0,1.5)d\mu d\sigma}
$$
<br />

This looks scarier than it is. The numerator just means to calculate the following for each combination of $\mu$ and $\sigma$

1. for each participant $i$ calculate the product of 
  - likelihood ($Normal(w_i|\mu,\sigma)$) and 
  - prior ($Normal(mu|3.5,1.5) Uniform(\sigma|0,1.5)$)
2. calculate the product of all values generated in 1.
  
This is not how the calculation is really performed (expect when one uses grid approximation). Instead, methods lake Lapalce approximation or MCMC are used to calculate this quantity.

The denominator is what is called the evidence, and it's main purpose is to insure that the posterior sums to 1.

## Prior predictive check

It is good predictive to check if the model with priors make sensible predictions, before one estimates the model parameters.
The goal is not to prior-predict data that are very similar to observed data, but to prior-predict data that pass plausibility checks and are in the same ball park as the observed data.
Plausibility checks are simple things like that there are not negative weights. With "in the same ball park" one means that the values should ave approximately the same order of magnitude. For instance, newborns are a few kilogram heavy, and not 10s or 100s of kilogram. Prior predictive checks rely on domain knowledge, and can be very useful in understanding the effect of multivariate priors. We still give it a quick try for our simple example:

```{r PriorPredictiveAlpha}
prior.pedictive.weights = 
  rnorm(
    10000,
    rnorm(10000, 3.5, 1.5),
    runif(10000, 0, 1.5))

hist(prior.pedictive.weights,
     main = "", xlab = "prior predictive weights")
text(-2,2000,expression(mu~"="~Normal(3.5,1.5)))
text(-2,1850,expression(sigma~"="~Uniform(3.5,1.5)))
```


This looks reasonable, even though we surely know that there are no newborns with a weight below 0 or above 6 kilogram. But the point of the prior is not to forbid impossible values. Such impossible values should be rare, and if they have to be allowed in order to insure a weak influence of the prior on the results, this is OK.


How does it look if we use non-informative priors?

```{r PriorPredictiveAlphaNonInformative, echo = F}
set.seed(1)
prior.pedictive.weights = rnorm(
  10000,
  rnorm(10000, 0, 1000),
  runif(10000, 0, 1000))

hist(prior.pedictive.weights, main = "", xlab = "prior predictive weights")
text(-3300,1500,expression(mu~"="~Normal(0,10000)))
text(-3300,1350,expression(sigma~"="~Uniform(0,10000)))
```

It seems obvious that the first set of parameters makes more sense.

## Estimating the model parameters

To analyse the model, we can just use the data.frame we created above:

```{r showData}
head(dt)
```

Based on the quap code in the table above, we can also put the quap model. `alist` is a command that creates a list that holds our model.

```{r alphaquapmpodel}
alpha.model.list =
  alist(
    weight ~ dnorm(mu,sigma),
    mu <- alpha,
    alpha  ~ dnorm(3.5,1.5),
    sigma  ~ dunif(0,1.5)
  )
```

For reasons that will be clear soon, we are using a parameter $\alpha$ as a determinent of the mean $\mu$.

This is what the model list looks like:

```{r alphashowquap}
alpha.model.list
```

Now we can use `quap` to calculate the posterior distribution.

```{r alphafit}
alpha.model.fit = quap(alpha.model.list, data=dt)
```


And we can have a first glimpse of the results with the `precis` function from the `rethinking package`:

```{r alphaprecis}
precis(alpha.model.fit)
```

## Posterior predictive check

Before we start interpreting our data, it is always a good idea to see if the model is any good at describing the data.

To do this, we first extract the posterior from the prior:

```{r alphaposterpred1}
alpha.model.post = extract.samples(alpha.model.fit,n=1e4)
alpha.model.post$mu = alpha.model.post$alpha
head(alpha.model.post)
```

One thing we would like to check is if the observed mean is within the credible interval of the posterior for $\mu$:

```{r alphaposterpred2}
hist(alpha.model.post$mu, main = "",xlab = expression(mu))
abline(v = HPDI(alpha.model.post$mu), col = "blue")
abline(v = mean(dt$weight), lwd = 2)
```

We also expect that most of the data lies within the posterior predicted values. First we calculate the posterior predictions, which depend on $\mu$ and $\sigma$:

```{r alphaposterpred5}
posterior.predictions = 
  rnorm(nrow(alpha.model.post),
        alpha.model.post$mu,
        alpha.model.post$sigma)

hist(dt$weight, col = "gray10", main = "", xlab = "weight")
hdpi = HPDI(posterior.predictions)
abline(v = hdpi , col = "blue")
in.hdpi = mean(dt$weight > hdpi[1] & dt$weight < hdpi[2])

title(paste0(round(in.hdpi*100),"% of weights are in the 89% HDPI"))

```

Now let's simulate 200 weights and see if they are associated with length:

```{r alphaposterpred3, fig.width=8, echo = F}
par(mfrow = c(1,2))
plot(dt,
     ylab = "weight",
     xlab = "length")

pp.weight = rnorm(250,
      alpha.model.post$mu[1:250],
      alpha.model.post$sigma[1:250])
plot(dt$length,pp.weight,
     ylab = "posterior prediction weight",
     xlab = "length",
     col = "blue")
```

Differently than in the observed data, the predicted data do not show an association between length and weight. This is obviously because we did not use length to predict weight.

# Linear regression

Instead of the previous model, where each individuals weight depends only on the group mean, we want a model in which individual weights also depend on birth length.


```{r alphaposterpred4, fig.width=8}
par(mfrow = c(1,2))
plot(dt,
     ylab = "weight",
     xlab = "length",
     ylim = ylim)
abline(h = sample(alpha.model.post$mu,100),
       col = adjustcolor("blue",alpha = .2))
xs = runif(100,39.75,40.25)
arrows(xs,rep(0,100),xs,
       sample(alpha.model.post$mu,250),
       col = adjustcolor("red",alpha = .1),
       code = 3)

plot(dt,
     ylab = "weight",
     xlab = "length",
     ylim = ylim)
abline(lm(dt$weight~dt$length), lty = 3, col = "blue", lwd = 3)
```

Looking at the right-hand side of the plot, we see that it is not so easy to think about what the mean weight is, length has a difficult scale. We can just re-scale it.

```{r scaledata, fig.width=6, fig.height=6}
dt$length.s = scale(dt$length)
plot(dt$length.s,
     dt$weight,
     ylab = "weight",
     xlab = "length",
     ylim = ylim)
```


Now it is easier to see that the when length = 0 (the average) weight should be around 3.5 kg, and that when the weight goes up by 5 units (-3 to 2), length goes up by 4 units (2 to 6), that `r 4/5`kg per cm length.

## The model

Next we "build" the new model, which can be visualized as follows:

```{r moModelPlot, fig.width=6, fig.height=6, echo = F}
plot(dt$length.s,
     dt$weight,
     col = "grey",
     ylab = "weight",
     xlab = "scaled length",
     ylim = ylim,
     xlim = c(-3,3))
coeffs = coef(lm(dt$weight~dt$length.s))
abline(coeffs, col = "blue", lwd = 2)
arrows(0,0,0,mean(dt$weight), code = 3, col = "red", lwd = 2)
abline(h = c(0,coeffs[1] + -3*coeffs[2]), col = "grey", lty = 3)
draw.arc(-3, coeffs[1] + -3*coeffs[2], 1, deg1 = 0, deg2 = coeffs[2]*45, lwd = 2, col = "magenta", n = 100)
text(0,1, expression(alpha), pos = 4, col = "red", cex = 1.5)
text(-2.2,coeffs[1] + -2.2*coeffs[2], expression(beta), pos = 1, col = "magenta", cex = 1.5)
text(2,coeffs[1] + 2*coeffs[2], expression(mu~"="~alpha+beta*l.s), pos = 1, col = "blue", cex = 1.5, srt=25.6)
```

Here is the full model

<br/>

| What | Notation | quap R-code |
|---|---|---|
|Likelihood | $w_i \sim Normal(\mu,\sigma)$ | `weight ~ dnorm(mu, sigma)` |
|Trans. paras. | $\mu_i = \alpha + \beta l_i$ | `mu[i] <- a + b * length.s[i]`|
|Prior | $\alpha \sim Normal(3.5,1.5)$ | `alpha ~ dnorm(3.5, 1.5)` |
|Prior | $\beta \sim Normal(1,1)$ | `beta ~ dnorm(1, 1)` |
|Prior | $\sigma \sim Uniform(0,1.5)$ | `sigma ~ dunif(0, 1.5)` |

<br/>

The only real difference to the previous model is in lines 2 and 4. 

## Prior predictive check

We do again a prior predictive check to see if model and prior are broadly in line with the data.

```{r mupriorpredicttive1, fig.width=7, fig.height=7}
plot(0,
     ylab = "weight",
     xlab = "scaled length",
     ylim = c(1,7),
     xlim = c(-3,3))
clr = adjustcolor("black",alpha = .25)
for (k in 1:50)
  abline(rnorm(1,3.5,1.5), rnorm(1,1,1), col = clr)
```

This looks pretty wild. It is implausible that we have a negative association between length and weight, and the mean weight covers an implausibly large range. We can do better than that, without that the model priors determine the fitting results.

```{r mupriorpredicttive2, fig.width=7, fig.height=7}
plot(0,
     ylab = "weight",
     xlab = "scaled length",
     ylim = c(1,7),
     xlim = c(-3,3))
for (k in 1:50)
  abline(rnorm(1,3.5,1), rlnorm(1,-.25,.5), col = clr)
```

This looks much more reasonable. Here is the model with new priors:

| What | Notation | quap R-code |
|---|---|---|
|Likelihood | $w_i \sim Normal(\mu,\sigma)$ | `weight ~ dnorm(mu, sigma)` |
|Trans. paras. | $\mu_i = \alpha + \beta l_i$ | `mu[i] <- a + b * length.s[i]`|
|Prior | $\alpha \sim logNormal(3.5, 1)$ | `alpha ~ dnorm(3.5, 1)` |
|Prior | $\beta \sim Normal(-.25, .5)$ | `beta ~ dlnorm(-.25, .5)` |
|Prior | $\sigma \sim Uniform(0,1.5)$ | `sigma ~ dunif(0, 1.5)` |

And here is the `quap` model:

```{r muquap}
mu.model.list =
  alist(
    weight ~ dnorm(mu,sigma),
    mu <- a + b * length.s,
    a  ~ dnorm(3.5,1),
    b  ~ dlnorm(-.25,.5),
    sigma  ~ dunif(0,1.5)
  )
```


## Estimating the model parameters

We fit the `quap` model.

```{r fitmumodel}
mu.model.fit = quap(mu.model.list, data=dt)
precis(mu.model.fit)
```

## Posterior predictive check

As should become custom, we check if our model describes the data reasonably well. We first extract the posterior samples. 

```{r mugetsamples}
mu.model.post = extract.samples(mu.model.fit)
```

Next we calculate the "transformed parameter" $\mu$ for each participant. Because we have `r nrow(mu.model.post)` posterior samples and `r nrow(dt)` participants, we end up with a `r nrow(mu.model.post)` time `r nrow(dt)` matrix of posterior predictions.

```{r}

```


```{r mucalcposterpred}
n_samples = nrow(mu.model.post)

get_mu.model.pp = function(x,posterior, type = "pred"){
  x = matrix(x, ncol = 1)
  mu.model.pp = 
    apply(
      x, # instead of a for loop
      1,  # for each row in dt
      function(length.s){
        with(data.frame(posterior),
             {
               mu = a + b * length.s             # calculate mu
               if (type == "lin_pred") {
                 return(mu)
               } else {
                 pp = rnorm(n_samples,mu,sigma)  # generate post. preds
                 return(pp)
               }
             }
        )
      }
    )
}

mu.model.pp = get_mu.model.pp(dt$length.s,mu.model.post)

dim(mu.model.pp)
```

And we check if the predicted weights are consistent with the observed means.
```{r muposterpred1, fig.width=8, fig.height=8, echo = F}
par(mfrow = c(2,2))
for (p in c(.1, .25, .5, .9)) {
  hist(dt$weight, col = "gray10", main = "", xlab = "predicted weight")
  hdpi = HPDI(as.vector(mu.model.pp),prob = p)
  abline(v = hdpi , col = "blue")
  in.hdpi = mean(dt$weight > hdpi[1] & dt$weight < hdpi[2])
  title(paste0(round(in.hdpi*100),
               "% of weights are in the ",p*100,"% HDPI"))
}
```

Posterior predictive checks do not need to be limited to simlpy the outcome variable. We can check if any aspect of the data we care about works. Here we check if mean and sd of the predicted data lie within the credible interval of the posterior.

```{r muposterpred2, fig.width=8 }
par(mfrow = c(1,2))

pp.mu = apply(mu.model.pp, 1, mean)
pp.sd = apply(mu.model.pp, 1 , sd)

hist(pp.mu, main = "")
abline(v = c(HPDI(pp.mu),mean(dt$weight)), 
       col = c("blue","blue","black"))

hist(pp.sd, main = "")
abline(v = c(HPDI(pp.sd),sd(dt$weight)), 
       col = c("blue","blue","black"))
```

We can also check if we see the covariation of birth length and birth weight of we use predicted weights.

```{r mupostpredcorr, fig.width=8, fig.height=8, echo = F}
par(mfrow = c(2,2))
for (k in 1:4) {
  plot(dt$length, mu.model.pp[k,], col = "blue",
       xlab = "length", ylab = "predicted weight",
       ylim = ylim)
}
```

## Describe the results

Now that we have convinced of the that the model describes the data well enough (yes, this is to a degree subjective) we can present our results.

As a visual presentation we can e.g. just show the slope:

```{r muresultsfigure1}
plot(dt$length.s,
     dt$weight,
     col = "grey", 
     ylab = "weight",
     xlab = "length",
     xaxt = "n")
x.ticks = seq(35,60,by = 5)
axis(1, 
     at = (x.ticks-mean(dt$length))/sd(dt$length), 
     labels = x.ticks)
for(k in 1:25) 
  abline(a = mu.model.post$a[k], 
         b = mu.model.post$b[k],
         col = adjustcolor("black",alpha = .2))
```

I like spaghetti plots, but confidence bands are more typically used:

```{r muresultsfigures2, fig.width=8, echo = F}
length.s.new = seq(min(dt$length.s),max(dt$length.s),.1)
mu.model.pp.nd = 
  get_mu.model.pp(length.s.new,
                  mu.model.post)
mu.model.lin_pred.nd = 
  get_mu.model.pp(length.s.new,
                  mu.model.post,
                  type = "lin_pred")

HDPI.pp = apply(mu.model.pp.nd, 2, HPDI)
HDPI.lin_pred = apply(mu.model.lin_pred.nd, 2, HPDI)

conf_band = function(x,y) {
  polygon(c(rev(x), x),
          c(rev(y[2,]), y[1,]),
          col = adjustcolor("blue", alpha = .1),
          border = NA)  
}

par(mfrow = c(1,2))
plot(dt$length.s,
     dt$weight,
     col = "grey", 
     ylab = "weight",
     xlab = "length",
     xaxt = "n",
     main = "Credible interval of expected weight")
conf_band(length.s.new,HDPI.lin_pred)
x.ticks = seq(35,60,by = 5)
axis(1, 
     at = (x.ticks-mean(dt$length))/sd(dt$length), 
     labels = x.ticks)
abline(mean(mu.model.post$a),mean(mu.model.post$b), col = "blue")

plot(dt$length.s,
     dt$weight,
     col = "grey", 
     ylab = "weight",
     xlab = "length",
     xaxt = "n",
     main = "Credible interval of predicted weight")
conf_band(length.s.new,HDPI.pp)
x.ticks = seq(35,60,by = 5)
axis(1, 
     at = (x.ticks-mean(dt$length))/sd(dt$length), 
     labels = x.ticks)
abline(mean(mu.model.post$a),mean(mu.model.post$b), col = "blue")
```


This is how one could describe the results:

We found that a on standard deviation increases in birth length was associated with a `r round(mean(mu.model.post$b)*1000)` (credible interval `r paste(round(HPDI(mu.model.post$b*1000)), collapse = ", ")`) increase with of birth weight.

# Splines

Splines are an alternative to polynomial regression if one wants to model non-linear relationships.

In polynomial regression, the regression weight for each basis functions changes the predicted values over the entire range of the predictor variables.

```{r polynomial, fig.height=7}
x = seq(-3,3,.1)
X.poly = poly(x, 2, raw = T)

b1 = c(1,1)
b2 = c(1,2)

y1 = X.poly %*% b1
y2 = X.poly %*% b2

par(mfrow = c(2,1))
matplot(X.poly, type = 'l', ylab = "basis function value", xlab = "" )
title("Polynomials")
plot(x, y1, 'l', lwd = 2,
     ylim = range(c(y1,y2)),
     col = "blue",
     ylab = "y")
lines(x,y2, col = "red", lwd = 2, lty = 2)

```

As a result, it is not possible to fit local trends.

Splines partition the x-axis in overlapping regions, and parameter for basis functions modify the predicted values only in specific regions.


```{r splines , fig.height=7}
library(splines)
knot.locations = seq(-3,3, length.out = 5)
X.splines = bs(x, knots = knot.locations)

b1 = scale(1:ncol(X.splines))^2
b2 = b1
b2[(length(b2)-2):length(b2)] = 0

y1 = X.splines %*% b1
y2 = X.splines %*% b2

par(mfrow = c(2,1))
matplot(X.splines, type = 'l', ylab = "basis function value", xlab = "")
title("Spline basis functions")
plot(x, y1, 'l', lwd = 2,
     ylim = range(c(y1,y2)),
     col = "blue",
     ylab = "y")
lines(x,y2, col = "red", lty = 2, lwd = 2)
```


As a result, splines can model local variations. 