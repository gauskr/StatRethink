---
title: "Chapter 7: Recap"
author: "Guido Biele"
date: "27.04.2022"
output:
  html_document: 
    mathjax: default
    toc: true
    toc_depth: 2
    code_folding: hide
header-includes: 
    \usepackage{xcolor}
    \usepackage{amsmath}
---

  
```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
}
pre{
  font-size: 20px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 22pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}
```

```{r setup, include=FALSE, message=FALSE, warning=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE, dpi = 300, global.par = TRUE)
library(rethinking)
library(magrittr)
library(knitr)
library(kableExtra)
library(MASS)
source("../utils.R")
```

```{r, echo = F}
par(mar=c(3,3,0,1), mgp=c(1,.5,0), tck=-.01)
```


# Shrinkage

Model comparison (done correctly) helps to choose the model that provides a good representation of the true DGP by penalizing models that "overfit". This penalization is achieved mainly by assessing "fit" not on a training data set, but on a hold out test data set.

A complementary approach to work against "overfitting" is to specify priors that shrink model coefficients towards zero. Such shrinkage priors are typically normally distributed, have a mean of zero and a _relatively_ small standard deviation. Here _relative_ refers to the scale on which a predictor is measured.

To show how shrinkage works, we estimate spline models with different standard deviations on regression coefficients for the simulated income / well being data above.

The following figure shows the estimated relationships for different samples drawn from the population.

![](shrinkage.gif)


Hopefully you can see that large deviations between the true DGP in red and the estimated DGP in blue are less frequent when the prior on regression coefficients is narrow (top left) compared to when it is wider (bottom right).


To confirm this, the following plot shows deviances from a simulation that 
- samples 1000 times training and test data, 
- estimates model parameters on the training set
- calculates deviances for the training and test data sets.

The following figure shows deviances +/- 1 sd.

```{r}
load("sim_lppd.Rdata")
D.test = -2*elpd.test
D.train = -2*elpd.train
m.test = colMeans(D.test)
m.train = colMeans(D.train)

within_sd = function(m) {
  apply(apply(m,2, function(x) x - rowMeans(m)), 2, sd)
}

lower.test = m.test - within_sd(elpd.test)
upper.test = m.test +  within_sd(elpd.test)
lower.train = m.train - within_sd(elpd.train)
upper.train = m.train + within_sd(elpd.train)

par(mfrow = c(2,1), mar=c(2.5,2.5,.5,.5), mgp=c(1,.1,0), tck=-.01)
ylim = range(c(lower.train, upper.train))
xs = 1:ncol(elpd.test)
plot(xs,m.train, ylim = ylim, 
     ylab = "Deviance train", xlab = "", xaxt = "n")
axis(1, at = 1:5, labels = c(1,2,5,10,20))
arrows(xs,y0 = lower.train, y1 = upper.train, length = 0)


ylim = range(c(lower.test, upper.test))
plot(xs+.2, m.test, col = "blue", pch = 16, ylim = ylim,
      ylab = "Deviance test", xlab = "sd(b)", xaxt = "n")
arrows(xs+.2,y0 = lower.test, y1 = upper.test, length = 0, col = "blue")
axis(1, at = 1:6, labels = b.sds)

#text(xs,m.train, labels = round(m.train,2), pos = 2, cex = .5)
#text(xs,m.test, labels = round(m.test,2), pos = 2, cex = .5, col = "blue")
```

Indeed, we see that while models with wider priors on the regression coefficients have a lower deviances in the training data, they have larger deviances for the test data. This becomes also clear, if we plot many estimated DGPs together:

```{r, fig.width=15, fig.height=10, echo = FALSE}
library(splines)
set.seed(11)
x = seq(-3,3,length.out = 250)
knot.locations = seq(-3,3, length.out = 10)
B = bs(x, knots = knot.locations)
b = rnorm(ncol(B))
pre.y = B %*% b
y = pre.y + rnorm(length(x),0,.5)
par(mfrow = c(2,3), mar=c(2.5,2.5,0,.5), mgp=c(1,.1,0), tck=-.01)
for (b.sd in b.sds) {
  plot(x,y, col = adjustcolor("black", alpha = .025))
  matlines(x,t(yhats[[which(b.sd == b.sds)]]), lty = 1, 
           col = adjustcolor("blue",alpha = .05))
  text(-4,1.2, paste0("b ~ normal(0, ",round(b.sd,1),")"), pos = 4)
  lines(x,pre.y, col = "red", lwd = 2)
}
```


<!-- Here is an example: Assume you want to estimate the effect of educational level (elementary school, high school, bachelor, master or more) and gender on income. -->

```{r, echo = FALSE, eval = FALSE}
X = expand.grid(edu = seq(-2,2,1), gender = c(0,1))
dt = do.call(rbind,lapply(1:4, function(x) X))
b = c(.75,.2,-.035,.1)
dt$y = model.matrix(~poly(edu,2,raw = T) + gender, dt) %*% b
ids = which(dt$edu == 2 & dt$gender == 1)
dt[ids[1],"y"] = dt[ids[1],"y"] + .75
dt = dt[-ids[3:4],]
dt$y = dt$y + rnorm(nrow(dt),0,.1)
plot(dt$edu,dt$y)
```

# Cross validation

So far we have implemented some simple cross validation manually, by simply simply splitting our total sample in half. However, data can also be split differently, e.g. 20% training data 80% test data or the other way around.

One popular way to split data to fit the data on $N-1$ data points and to use the $N$th data point as test data. This is referred to as _Leave one out cross validation_ or LOOCV.

One thing that is easily implemented with LOOCV is to calculated the deviance as the average deviance over the $N$ LOOCV deviances. for LOOCV, there are always only N-1 training data sets.

This is different for alternative schemes. For instance, at a samples size of 20, there are `r choose(20,10)` to construct the training data set. Here, averaging about all possible test-data deviances would be too computationally expensive.

The key thing to keep in mind when doing cross validation is that dependent observations (more specificall, observations with correlated errors) should always be kept in either test or training data sets. Such complications plays e.g. a role in time series or hierarchically organized data sets.

## LOO-CV and PSIS-LOO

The computationally challenging part of LOO-CV is that one needs to estimate the model $N-1$ times.

Fortunately, one can approximate LOO-CV with a method called pareto smoothed importance sampling **PSIS-LOO**. Here, not all $lppd$ receive the same weight when calculating the deviance, but 

- $lppd$ values of observations that have a strong influence receive a higher weight and 
- pareto-smoothing (of the tails of the $lppd$ distribution) is used to get better weights.

# Information criteria

The goal of information criteria is to give us the information out of sample prediction (cross validation) gives us, without that we need to split the data into test and training set.

Information criteria are calculated from two quantities

- the deviance 
- a penalty term

All well known information criteria, like **AIC**, **DIC**, **WAIC** use the same deviance term, but they differ in their penalty:

- AIC: number of parameters
- BIC: number of parameters and sample size
- DIC: variance of deviances (effective # parameters)
- WAIC: variance of posterior predictions (effective # parameters)


Never heard of WAIC, why should you used it?

- fewer requirements (about posterior distribution, relative weight of prior and likelihood, number parameters vs sample size)
- can provide warning signals if they are unreliable
- allow calculation of standard errors, which puts differences between models in perspective


## Are model comparison criteria any good?

Benchmark is out of sample prediction or cross validation, where we draw first a training sample from the DGP, estimate model parameters, draw a test sample (of same size) and estimate the out of sample deviance as our parameter of interest.

In the following figure, these **cross validation* values are represented by dots (empty for flat priors black for shrinkage priors).

Lines in the figures are _average values_ over 1000 simulations and show that on average information criteria, **(LOO)-CV**, **PSIS-LOO** and **WAIC** do a good job of approximating full cross validation. 

![](criteria_sim.PNG)

When looking at averages, positive and negative differences to full corss validation can cancel. Therefore, it is good to also look at the average absolute, i.e. for each simulated data set we calculate the absolute of the differences between cross validation llpd and the approximation. This is shown in the following figure:

![](criteria_sim_error.PNG)

Bottom line: **PSIS-LOO** and **WAIC** are relatively close to full cross validation and can be computed relatively cheaply. In practice, it is good to use both and take discrepant results are a warning signal.


