---
title: "Chapter 7: Recap"
author: "Guido Biele"
date: "27.04.2022"
output:
  html_document: 
    mathjax: default
    toc: true
    toc_depth: 2
    code_folding: hide
header-includes: 
    \usepackage{xcolor}
    \usepackage{amsmath}
---

  
```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
}
pre{
  font-size: 20px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 22pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}
```

```{r setup, include=FALSE, message=FALSE, warning=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE, dpi = 300, global.par = TRUE)
library(rethinking)
library(magrittr)
library(knitr)
library(kableExtra)
library(MASS)
source("../utils.R")
```

```{r, echo = F}
par(mar=c(3,3,0,1), mgp=c(1,.5,0), tck=-.01)
```

One important goal of statistical inference is to learn about the data generating process. In the simple case, this can simply mean to find out if a certain independent variable influences our outcome of interest. If we do a randomized experiment, we can just compare the outcomes conditional on different values of the independent variable to find out if the independent variable is part of the data generating process.

Keeping with our experiment example, we can also describe two alternative statistical models (e.g. regression) that describe the data generating process, where one model assumes an effect of the independent variable, but the other does not. Then we could compare the two models and declare the model that fits the data better to be the winner. The problem with this approach is that if the addition of one independent variable is the only difference between the two models, the model with more variables will always fit the data better, even if it is not the correct model.

Hence, we need more sophisticated approaches to determine which of two models better capture the data generating process.

Note that we are not interested in finding a model that describes the data we are seeing well, then we could just choose the best fitting model. Instead, we want to use the data at hand to learn about the process that generated the data we see. After all, only knowledge about this process generalizes to other situations.

# Overfitting and underfitting

The fact that more complex models always fit the data better is related to the term _overfitting_. Overfitting means that the fitted model does not only describe the data generating process, but also takes part of the data that is just noise to be part of the data generating model.

For instance, assume that we are looking at the relationship between income and well being, which could have this (simulated) relationship:

```{r}
N = 1000
set.seed(123456)
x = sort(rnorm(N,mean = 0))
bf.s = splines::bs(x,knots =3)
bf.b = matrix(c(1,1,1,0),ncol = 1)
mu = bf.s %*% bf.b
y = rnorm(N, mu , sd = .1)
par(mfrow = c(1,1))

plot_xy = function(x,y,mu, idx = NULL) {
  if (is.null(idx)) idx = 1:length(x)
      plot(x[idx],y[idx], 
           ylab = "",
           xlab = "",
           xlim = quantile(x,c(.01,.99)), 
           ylim = quantile(y,c(.01,.99)))
      lines(x,mu,col = "red")
    }

plot_xy(x,y,mu)


# N = 500
# x = rnorm(N,mean = 0)
# y = rnorm(N,b1*x - b2*x^2, sd = 1)
# par(mfrow = c(1,1))
# plot(x,y)
# lines(sort(x),b1*sort(x) - b2*sort(x)^2, col = "red")
```

However, we only have a sample of `N = 15` participants to learn about this relationship.
The following figure shows different random samples in rows, and predictions of regression models of increasing complexity, from a simple linear model $\small \mu = \alpha + \beta x$ to a polynomial of the 4th order $\small \mu = \alpha + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4$,  in columns.
The red lines show the true relationship and the blue lines show the learned relationships.

```{r}
plot_fits = function(x,y,mu,sample_size) {
  par(mfrow = c(4,4), mar=c(2,2,.5,.5), mgp=c(1.25,.25,0), tck=-.01)
  for (k in 1:4) {
    idx = sample(N,sample_size)
    
    dt = data.frame(x = x[idx], y = y[idx])
    
    q.model = alist(
      y ~ dnorm(mu,exp(log_sigma)),
      mu <- a + b*x,
      a ~ dnorm(0.5,1),
      b ~ dnorm(0,10),
      log_sigma ~ dnorm(0,1)
    )
    
    plot_xy(x,y,mu,idx)
    plot_quap_preds(q.model,dt,"x", plot = F)
    
    plot_xy(x,y,mu,idx)
    q.model[[2]]  = alist(mu <- a + b[1]*x + b[2]*x^2)[[1]]
    plot_quap_preds(q.model,dt,"x", start.list = list(b=rep(0,5)), plot = F)
    
    plot_xy(x,y,mu,idx)
    q.model[[2]]  = alist(mu <- a + b[1]*x + b[2]*x^2 + b[3]*x^3)[[1]]
    plot_quap_preds(q.model,dt,"x", start.list = list(b=rep(0,5)), plot = F)
    
    plot_xy(x,y,mu,idx)
    q.model[[2]]  = alist(mu <- a + b[1]*x + b[2]*x^2 + b[3]*x^3 + b[4]*x^4)[[1]]
    plot_quap_preds(q.model,dt,"x", start.list = list(b=rep(0,5)), plot = F)
    
  }
}
plot_fits(x,y,mu,sample_size = 15)
```

This figure shows that while making the model increasingly complex allows us to fit the data ever better (see the $\small R^2$ values), it does not necessarily allows us to capture the true data generating process better.


What happens if we increase the sample size to 75?

```{r}
plot_fits(x,y,mu,sample_size = 75)
```


The variation in the inferred associations between different samples is now smaller, but we still see overfitting and also underfitting, i.e. the inability of the simplest model to capture the non-linear relationship.

As an aside, we can also see that a polynomial model has difficulties in capturing the true non-linear and monotonic relationship, which was generated with a spline model.


The goal of model comparison is then to identify the model that best describes the data generating process, i.e. that does suffer the least from over of underfitting.

# Evaluating accuracy

To identify which model is good, we need some measure or model accuracy, or consistency of model and data.

Imagine, you are working with a drug rehabilitation center and trying to evaluate different statistical models that predict if a patient will have a relapse within 4 weeks after leaving the center. Your data look as follows:

```{r}
set.seed(1234)
treatment.weeks = round(rnorm(15,mean = 3, sd = 1),1); 
dt = data.frame(
  treatment.weeks, 
  p.relapse = round(inv_logit(-treatment.weeks+1.25),2), 
  relapse = (round(inv_logit(-treatment.weeks+1.25),2) > runif(15))*1 )
kable(dt[, c("treatment.weeks", "relapse")]) %>% 
  kable_styling(full_width = F)
```

We can build 2 predictions models, one which takes the number of treatment weeks into account, and one that just assumes that treatment is good and nobody has a relapse. Here is the data with predictions from these models:

```{r}
dt$pred.no_relapse = 0
dt$pred.weeks = 
  glm(dt$relapse~dt$treatment.weeks, family = binomial()) %>% 
  predict(type = "response") %>% 
  round(3)

kable(dt[, c("treatment.weeks", "relapse", "pred.no_relapse", "pred.weeks")]) %>% 
  kable_styling(full_width = F)
```

A simple way to calculate accuracy would be to compute the average probability to make a correct prediction. Here are these average probabilities for the two models:

```{r, class.source = "fold-show"}
av_acc.no_relapse = 
  (sum(1-dt$pred.no_relapse[dt$relapse == 0]) +   # prob. non-relapse
     sum(dt$pred.no_relapse[dt$relapse == 1])) /  # prob. relapse
  15                                              # divide by N 
av_acc.weeks = 
  (sum(1-dt$pred.weeks[dt$relapse == 0]) + 
   sum(dt$pred.weeks[dt$relapse == 1]))/15

av_acc.no_relapse
av_acc.weeks
```

We can see that, by this metric, the model that ignores additional information and the true data generating process performs better.
The ability to favor models that are inconsistent with the data generating process is an undesirable property of an accuracy criterion.

Recall that in Bayesian updating we used the joint probability of the data given the model, which is something different than the average probability we just calculated. One intuition of why the joint probability is useful is that because it involves multiplication of all probabilities, it does not allow us to ignore some of the data as long as we predict most of the data well. Yet another view point is that the averaging allows us to favor models that make impossible predictions, like the probability of relapse is zero for everyone.

Hence, here is the joint probability for our two models:

```{r, class.source = "fold-show"}
joint_prob.no_relapse = 
   prod(1-dt$pred.no_relapse[dt$relapse == 0]) *  
     prod(dt$pred.no_relapse[dt$relapse == 1])
joint_prob.weeks = 
  prod(1-dt$pred.weeks[dt$relapse == 0]) * 
   prod(dt$pred.weeks[dt$relapse == 1])
joint_prob.no_relapse
joint_prob.weeks
```

Now we see that the model that is consistent with the data generating process has a better "score". Compare the code for the average accuracy and the joint probability to see that the difference is that the former approach sums over probabilities whereas the latter multiplies.

Instead of multiplying probabilities, we can also sum log-probabilities:

```{r, class.source = "fold-show"}
joint_log_prob.no_relapse = 
   sum(log(1-dt$pred.no_relapse[dt$relapse == 0])) +  
     sum(log(dt$pred.no_relapse[dt$relapse == 1]))
joint_log_prob.weeks = 
  sum(log(1-dt$pred.weeks[dt$relapse == 0])) + 
   sum(log(dt$pred.weeks[dt$relapse == 1]))
joint_log_prob.no_relapse
joint_log_prob.weeks
```

Multiplying probabilities and adding log probabilities leads to the same results that differ only due to numerical problems that arise when multiplying small probabilities:

```{r, class.source = "fold-show"}
joint_prob.weeks - exp(joint_log_prob.weeks)
```


# Information, uncertainty and entropy

The concepts of information and uncertainty are tightly related: The more information we have, the less uncertain we are.

Measures of information or uncertainty should have following properties:

- continuity
- increases as number of possible events increases
- additivity

## Entropy

We'll use the definition of entropy to display these properties. The entropy of a probability distribution encoded in the vector $\small p$ with length $\small n$ is defined as:

$$
H(p) = -E\;log(p_i) = \sum_{i=1}^{n} p_i log(p_i)
$$

Here, $\small E$ is the expectation or the weighted mean, where the weights are given by the probability for the events.

In `R` code we can write:

```{r  class.source = "fold-show"}
H = function(p) -sum(p*log(p))
```


## Continuity

Now lets look at the simple case of only two possible events. The following plot shows on the x-axis the probability of the first event $\small p_1$ and on the y axis the entropy:

```{r, fig.width=6, fig.height=6}
p1 = seq(.01,.99, length.out = 99)
entropy = do.call(
  c,
  lapply(p1, function(p1) H(c(p1,1-p1)))
)
layout(matrix(c(1,2,2), ncol = 1))
par(mar=c(0,3,1,1), mgp=c(1.25,.3,0), tck=-.01)
plot(0,type = "n", xlim = c(0,1), ylim = c(0,1), 
     xlab = "", xaxt = "n",
     ylab = expression(p[1]+p[2]))
polygon(x = c(0,1,1,0), y = c(0,0,1,0), col = "blue")
polygon(x = c(0,1,0,0), y = c(0,1,1,0), col = "red")
text(.75,.25, expression(p[1]), cex = 2, col = "white")
text(.25,.75, expression(p[2]), cex = 2, col = "white")
par(mar=c(3,3,1,1), mgp=c(1.25,.3,0), tck=-.01)
plot(p1,entropy, type = 'l', xlab = expression(p[1]))
```

The figure shows that when we have two events and assign them equal probability, then uncertainty/entropy are large, whereas uncertainty/entropy become smaller when we assign one of the tow options a higher probability.

All changes in entropy are continuous in changes of $\small p_1$, i.e. they do not have a step function.

## Increasing in number of events

What happens with entropy if we increase the number of events?

```{r}
n = 2:8
entropies = 
  do.call(
    c,
    lapply(n, function(n) H(rep(1,n)/n))
  )
names(entropies) = n
barplot(entropies, ylab = "entropy", xlab = "number possible events with equal probability")
```

As the number of possible events increases, entropy increases.

A related property is that if we start with probabilities for two events, e.g.

```{r class.source = "fold-show"}
P2 = c(A1 = .5, A2 = .5)
```

and further split one of the two events

```{r class.source = "fold-show"}
P2s = c(A1 = .5,  A2.1 = .25, A2.2 = .25)
```

than the entropy of the second distribution / model should be larger:

```{r class.source = "fold-show"}
cbind(H(P2), H(P2s))
```


## Additivity

Lets assume we have the following distributions of event types A and B:

```{r}
P = matrix(c(.125,.375,.125,.375), ncol = 2)
colnames(P) = c("A1","A2")
rownames(P) = c("B1","B2")

P %>% 
  kable() %>% 
  kable_styling(full_width = F)
```

Given these cells, we can calculate the margins:

```{r}
P = addmargins(P)
P %>% 
  kable() %>% 
  kable_styling(full_width = F)
```

And given the margins, we can also calculate the cell entries. For example:

$$
\small
\begin{aligned}
 P(A=1, B=1) & =P(A=1)P(B=1)&\\
     & =.5 \cdot .25&\\
     & =.125
\end{aligned}
$$

Given that we can calculate the table cells from the margins and vice versa, the information in the cells and the margins should be the same, i.e.

$$
\small
\begin{aligned}
P_{cells}   = \{&P(A=1,B=1), \\ &P(A=1,B=0), \\&P(A=0,B=1),\\ &P(A=0,B=0)\}\\
P_A  = \{&P(A=1),\\ &P(A=0)\} \\
P_B  = \{&P(B=1),\\ & P(B=0)\} \\
H(P_{cells}) = &H(P_A) + H(P_B )
\end{aligned}
$$

Let's try this out: 

```{r class.source = "fold-show"}
H_cells = H(c(.125, .125, .375, .375))
H_margins = H(c(.5,.5)) + H(c(.25,.75))
cbind(H_cells, H_margins)
```

To summarize, you can keep in mind that
$$
entropy = uncertainty = -information
$$

# Using entropy to measure accuracy

To understand why entropy is useful to measure the accuracy of a model, one can reframe the problem and ask if a model contains the same information as the true data generating process. We can talk about information in the (model of the) data generating process and the (inferred) model, because we can calculate the probability of the data both for the true data generating process and the model  (what we create with the `sim` function from rethinking packages).

Lets use our initial example again to make clear what we mean when we measure accuracy:

- We assume there is a true DGP and data derived from it
- We sample from the population data and estimate a model to infer a candidate model
- We want to estimate the accuracy of our model, i.e. the deviation between the true DGP and the inferred candidate model

```{r, fig.width=10.5, fig.height=10.5}
layout(matrix(c(1,3,2,3), ncol = 2))
plot(x,y)
lines(x, mu, col = "red")
title("DGP and population data")
set.seed(1)
idx = sample(N,25)

dt = data.frame(x = x[idx], y = y[idx])

q.model = alist(
  y ~ dnorm(mu,exp(log_sigma)),
  mu <- a + b[1]*x + b[2]*x^2,
  a ~ dnorm(0.5,1),
  b ~ dnorm(0,10),
  log_sigma ~ dnorm(0,1)
)

plot_xy(x,y,mu, idx)
plot_quap_preds(q.model,dt,"x", plot = F, start.list = list(b = rep(0,3)))
mtext(1,text = "x", line = 2.5)
mtext(2,text = "y", line = 2.5)
title("DGP, sample & inferred DGP / model")

preds = plot_quap_preds(q.model,dt,"x", plot = F,
                       start.list = list(b = rep(0,3)),
                       return.yhat = TRUE)
preds$y = predict(bf.s,newx = preds$x) %*% bf.b 
plot(preds$x, preds$yhat,'l', col = "blue", ylab = "", xlab = "")
mtext(1,text = "x", line = 2.5)
mtext(2,text = "y", line = 2.5)
arrows(x0 = preds$x, y0 = preds$y, 
       y1 = preds$yhat,length = .05, code = 3, 
       col = adjustcolor("purple",alpha = .5))
lines(preds$x, preds$y,'l', col = "red")
lines(preds$x, preds$yhat,'l', col = "blue")
title("Difference between true DGP and candidate model")
legend("topleft", lty = 1, col = c("red","blue"), legend = c("true DGP", "model"), bty = "n")
```


We can calculate the probability of data according to the true DGP and the inferred model. In our example, the probability of the data given the true DGP is 
$$
\small y_i \sim Normal(1\cdot bs_1(x_i) + 1\cdot bs_2(x_i) + 1\cdot bs_3(x_i) + 0\cdot bs_4(x_i), 0.1)
$$^[$bs$ are spline basis functions]

or in `R` code:

```{r, eval = F, class.source = "fold-show"}
p = dnorm(y, 1*bs2 + 1*bs2 + 1*bs3 + o*bs4, 0.1)
```

As before, we can use `p` to calculate how much information about the data we have in the DPG. Entropy tells us how much information about the observed data we have in our model. 

The probability of the data given the inferred model is (using posterior means)
$$
\small y_i \sim Normal(0.97 + 0.11 \cdot x_i - 0.05 \cdot x_i^2, 0.1)
$$

or in `R` code:

```{r, eval = F, class.source = "fold-show"}
p = dnorm(y, 0.97 + 0.11*x - 0.05*x^2, 0.1)
```

If we want then to learn about the accuracy of a candidate model, we can compute how much information about the data it has, relative to the true DGP (which is also a model). This differences or deviance in information about the data is called the _Kullback-Leibler divergence_ or KL divergence or short $\small D_{KL}$.

With probabilities of observed events given the true DGP $\small p$ and model predicted probabilities  $\small q$ the _Kullback-Leibler divergence_ is defined as:
$$
\small
\begin{aligned}
D_{KL}(p,q) & = \sum_i p_i \,log(p_i) - p_i \, log(q_i) \\
       & = \sum_i p_i \, \left(log(p_i) - log(q_i)\right) \\
       & = \sum_i p_i \, log \left( \frac{p_i}{q_i} \right)
\end{aligned}
$$

Here, $\small p_i \,log(p_i)$ is the entropy and $\small p_i \,log(q_i)$ is the cross entropy. Also note that when $\small p_i = q_i$ then $\small log(p_i/q_i) = log(1) = 0$, so that $\small D_{KL}(p,q) = 0 \; | \; p = q$. 

Here is an R function for  the _Kullback-Leibler divergence_:

```{r  class.source = "fold-show"}
dKL = function(p,q) sum(p*(log(p)-log(q)))
```

which we can use to plot the divergence between the distribution `p = c(.375,.625)` and alternative distributions `q`:

```{r}
p = c(.375,.625)
q1 = seq(.01,.99,.01)
DKL_pq = do.call(
  c,
  lapply(q1, function(q1) dKL(p,c(q1,1-q1)))
)
plot(q1,DKL_pq,
     type = 'l',
     ylab = expression(D[KL](p,q)),
     xaxt = "n", bty = "n")
lines(rep(p[1],2), c(0, max(DKL_pq)), lty = 2)
axis(1, pos=0, at=seq(0,1,.25))

```

## Estimating deviance

So far, we have determined that 

- entropy is a good measure of information and 
- KL divergence is a good measure of distance between the true model and the model predictions.

However, we generally do not know the true model, we only have a sample of data that was generated from it. This is not really a problem, because our initial goal was model comparison. So what we really want to do, is to compare the deviances of two models, here $\small q$ and $\small r$:

$$\small D_{KL}(p,q) - D_{KL}(p,r)$$ 
Here is how the deviances of the tow models are defined:

$$
D_{KL}(p,q)  = \sum_i p_i \,log(p_i) - p_i \, log(q_i) \\
D_{KL}(p,r)  = \sum_i p_i \,log(p_i) - p_i \, log(r_i)
$$

If we want to compute the relative accuracy of the models, we calculate 

$$
relative \; accuracy = \sum_i p_i \,log(p_i) - p_i \, log(q_i) - \sum_i p_i \,log(p_i) - p_i \, log(r_i)
$$

Luckily, we can simplify this:

- Both deviances sum over $p_i \,log(p_i)$ and 
- multiply the probability of the data with $p_i$. 

Therefore we can remove these terms and are then in the delightful situation that we only need to know about $\small \sum_i log(q_i)$ and $\small \sum_i log(r_i)$ if we want to compare candidate models:

$$
\sum_i log(q_i) - \sum_i log(r_i)
$$

where 

$$
S(q) = \sum_i log(q_i)
$$

is the total score for model $\small q$.

To see how we can calculate this quantitu for a model, remember that we started with the definition of entropy:

$$
H(p) = -E\;log(p_i) = \sum_{i=1}^{n} p_i log(p_i)
$$

where $\small p_i$ is the probability of observing some data given the model (and its parameters). Where have we heard this before?

$$
\overset{\color{violet}{\text{posterior probability}}}{P(parameter|data)} = \frac{\overset{\color{red}{\text{likelihood}}}{P(data|parameter)} \cdot \overset{\color{blue}{\text{prior probability}}}{P(parameter)}}{\overset{\color{orange}{\text{evidence}}}{P(data)}}
$$

This is just the likelihood term we already use when we do Bayesian updating. One important thing when computing this quantity in a Bayesian context is that the $paramter$ is not a single value, but a posterior distribution.

The `rethinking` package has a function `lppd`, which calculates for each data point the log of the mean probability of the observation given the model and posterior distribution of parameters, also called the __log pointwise predictive density__.

Coming back to this example:

```{r class.source = "fold-show"}
q.model = alist(
  y ~ dnorm(mu,exp(log_sigma)),
  mu <- a + b[1]*x + b[2]*x^2,
  a ~ dnorm(0.5,1),
  b ~ dnorm(0,10),
  log_sigma ~ dnorm(0,1)
)
q.fit = quap(q.model, dt, start = list(b = rep(0,2)))
```

```{r, echo = F}
plot_xy(x,y,mu, idx)
plot_quap_preds(q.model,dt,"x", plot = F, start.list = list(b = rep(0,3)))
mtext(1,text = "x", line = 2.5)
mtext(2,text = "y", line = 2.5)
title("DGP, sample & inferred DGP / model")
```


We are calculating 

- for each data point its average probability (averaging over posterior samples) 
- compute the log of this probability
- sum over the log probabilities of all data points

```{r class.source = "fold-show"}
lppd(q.fit)
```

## Cross validation

So far, we have calculated the divergence for the data we observed. The problem is that this is thus still a divergence in model fit to the observed data. However, what we want to compare is how well to different models are consistent with the data generating process, or the data we can observe from it.

One pragmatic way to go about this is to evaluate the model fit not on the data we used to fit the model parameters (_training data_) but on a new set of _test data_ which we did not use to estimate parameters.

The following figure shows in addition to 25 data points use to estimated the model parameters 25 new test data points that were not used to estimate model parameter. To estimate the deviance of the model predictions from these tests data. 

To visualize that we are using the posterior distribution to calculate deviance, the figure also shows 25 lines (`mu`), one for each posterior sample. The figure does not show variations in the standard deviation.

```{r}
par(mar=c(3,3,0.5,0.5), mgp=c(1.25,.5,0), tck=-.01)
plot_xy(x,y,mu, idx)
mtext(1,text = "x", line = 2)
mtext(2,text = "y", line = 2)
set.seed(123)
test.idx = sample(setdiff(1:length(x),idx),25)
mu.test = link(q.fit, data = data.frame(x = c(x[test.idx],seq(-2.5,2.5,length.out = 100))), n = 50)
matlines(seq(-2.5,2.5,length.out = 100),y = t(mu.test[,-(1:25)]), lty = 1, 
         col = adjustcolor("blue", alpha = .25))
os = seq(-.02,.02, length.out = 50)
for (k in 1:50)
  arrows(x[test.idx] + os[k], y0 = y[test.idx], y1 = mu.test[k,1:25], length = 0, col = adjustcolor("purple", alpha = .2))
points(x[test.idx],y[test.idx], col = "red", pch = 16)
```

```{r, eval = F, echo = F}
for (N in c(20, 100)) {
  kseq = 1:5
  dev = sapply(kseq,function(k){
    print(k);
    r =  mcreplicate(1e4,sim_train_test(N=N,k=k),mc.cores=4);
    c( mean(r[1,]),mean(r[2,]),sd(r[1,]),sd(r[2,]))
  } )
  save(dev,file = paste0("dev_N",N,".Rdata"))
}
```

We can use the following figure from the book to explain why deviance in the training data set (cross validation) is a better approach to model comparison:

- deviation in the test data generally decreases with increasing model complexity, but we don't want to choose the most complex model
- deviance on a hold-out test data set is better able to identify the correct model (with 2 parameters)
- using deviance only on test data is also a problem for larger sample sizes.

```{r echo = F, fig.height=5, fig.width=10}
dev_plotter = function(dev) {
  par(mar=c(3,3,2,1), mgp=c(1.5,.5,0), tck=-.01)
  train = dev[1,]
  train.lower = train - dev[3,]
  train.upper = train + dev[3,]
  test = dev[2,]
  test.lower = test - dev[4,]
  test.upper = test + dev[4,]
  train.x = 1:5
  test.x = train.x+.1
  plot(train.x,train,
       xlim = c(0.75,5.25),
       xlab = "number of parameters",
       ylab = "deviance",
       ylim = range(c(train.lower,train.upper,test.lower,test.upper)))
  arrows(x0 = train.x, y1 = train.lower, y0 = train.upper, length = 0)
  arrows(x0 = test.x, y1 = test.lower, y0 = test.upper, length = 0, col = "purple")
  points(test.x,test, pch = 16, col = "purple")
  axis(1, at = 2, col = "red", col.axis = "red")
}
par(mfrow = c(1,2))
load("dev_N20.Rdata")
dev_plotter(dev)
title("N = 20")
load("dev_N100.Rdata")
dev_plotter(dev)
title("N = 100")
legend("topright", pch = c(1,16), 
       col = c("black","purple"), 
       legend = c("training","test"), bty = "n")
```

